import cv2
import numpy as np
import time
import argparse
import pyorbbecsdk
from pyorbbecsdk import Pipeline, Config, OBSensorType, OBAlignMode, FrameSet
from ultralytics import YOLO
from utils import frame_to_bgr_image

pipeline = Pipeline()
device = pipeline.get_device()
device_info = device.get_device_info()
device_pid = device_info.get_pid()
config = Config()

color_profile = pipeline.get_stream_profile_list(OBSensorType.COLOR_SENSOR).get_default_video_stream_profile()
config.enable_stream(color_profile)

depth_profile = pipeline.get_stream_profile_list(OBSensorType.DEPTH_SENSOR).get_default_video_stream_profile()
config.enable_stream(depth_profile)

print(f"Color profile: {color_profile.get_width()}x{color_profile.get_height()} @ {color_profile.get_fps()} FPS")
print(f"Depth profile: {depth_profile.get_width()}x{depth_profile.get_height()} @ {depth_profile.get_fps()} FPS")

config.set_align_mode(OBAlignMode.HW_MODE)
pipeline.enable_frame_sync()
pipeline.start(config)

model = YOLO("weights/bestn_rknn_model/bestn_rknn_model")  #bestn (nano) –∏–ª–∏ best (small)

TRIGGER_GESTURE = "jumbo"
RESET_GESTURE = "heart"
tracked_hand = None
tracked_trajectory = []
last_inference_time = 0  # –¢–∞–π–º–µ—Ä –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ 1 –∫–∞–¥—Ä –≤ —Å–µ–∫—É–Ω–¥—É
max_miss_frames = 60  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤ –¥–ª—è —Å–±—Ä–æ—Å–∞ —Ç—Ä–µ–∫–∞
missed_frames = 0  # –°—á—ë—Ç—á–∏–∫ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤
last_tracked_bbox = None  # –ü–æ—Å–ª–µ–¥–Ω–∏–π –±–æ–∫—Å —Ä—É–∫–∏
last_tracked_trajectory = []  # –ü–æ—Å–ª–µ–¥–Ω—è—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è –¥–≤–∏–∂–µ–Ω–∏—è


def detect_and_track(color_frame, depth_frame):
    global tracked_hand, tracked_trajectory, last_inference_time, missed_frames, last_tracked_bbox, last_tracked_trajectory

    current_time = time.time()
    if current_time - last_inference_time >= 1:  # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å —Ä–∞–∑ –≤ —Å–µ–∫—É–Ω–¥—É
        last_inference_time = current_time
        results = model(cv2.resize(color_frame, (640, 640)))

        hands = []
        for result in results:
            for box in result.boxes:
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                label = model.names[int(box.cls)]
                conf = float(box.conf)

                hands.append({
                    "bbox": (x1, y1, x2, y2),
                    "label": label,
                    "confidence": conf
                })

        if tracked_hand is None:
            for hand in hands:
                if hand["label"] == TRIGGER_GESTURE:
                    x1, y1, x2, y2 = hand["bbox"]
                    depth_value = depth_frame[y1:y2, x1:x2].mean()
                    tracked_hand = {"bbox": hand["bbox"], "depth": depth_value}
                    tracked_trajectory = []
                    missed_frames = 0
                    last_tracked_bbox = hand["bbox"]  # –ö—ç—à–∏—Ä—É–µ–º –±–æ–∫—Å
                    last_tracked_trajectory = []  # –û—á–∏—â–∞–µ–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é
                    print(
                        f"‚úÖ –ó–∞—Ö–≤–∞—á–µ–Ω–∞ —Ä—É–∫–∞ —Å –∂–µ—Å—Ç–æ–º {TRIGGER_GESTURE} - –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã: {x1, y1, x2, y2}, –ì–ª—É–±–∏–Ω–∞: {depth_value:.2f}")
                    break
        else:
            # –ü–æ–∏—Å–∫ —Å–∞–º–æ–π –±–ª–∏–∑–∫–æ–π —Ä—É–∫–∏ –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –≥–ª—É–±–∏–Ω–µ
            closest_hand = None
            min_depth_diff = float("inf")

            for hand in hands:
                x1, y1, x2, y2 = hand["bbox"]
                depth_value = depth_frame[y1:y2, x1:x2].mean()

                depth_diff = abs(depth_value - tracked_hand["depth"])
                if depth_diff < min_depth_diff:
                    min_depth_diff = depth_diff
                    closest_hand = hand

            if closest_hand and min_depth_diff < 400 and closest_hand["label"] != RESET_GESTURE:
                tracked_hand["bbox"] = closest_hand["bbox"]
                tracked_hand["depth"] = depth_frame[closest_hand["bbox"][1]:closest_hand["bbox"][3],
                                        closest_hand["bbox"][0]:closest_hand["bbox"][2]].mean()
                x1, y1, x2, y2 = tracked_hand["bbox"]
                tracked_trajectory.append(((x1 + x2) // 2, (y1 + y2) // 2))
                missed_frames = 0

                last_tracked_bbox = tracked_hand["bbox"]  # –ö—ç—à–∏—Ä—É–µ–º –±–æ–∫—Å
                last_tracked_trajectory = tracked_trajectory  # –ö—ç—à–∏—Ä—É–µ–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é

                print(f"üîµ –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º —Ä—É–∫—É - –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã: {x1, y1, x2, y2}, –ì–ª—É–±–∏–Ω–∞: {tracked_hand['depth']:.2f}")
                '''
                –∑–¥–µ—Å—å –º–æ–∂–Ω–æ –ø—Ä–æ–ø–∏—Å–∞—Ç—å –∫–µ–π—Å—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–≥–∏—è –∂–µ—Å—Ç–æ–≤
                '''

            else:
                missed_frames += 1
                if missed_frames > max_miss_frames or closest_hand["label"] != RESET_GESTURE:
                    print("‚ùå –†—É–∫–∞ –ø–æ—Ç–µ—Ä—è–Ω–∞, —Å–±—Ä–∞—Å—ã–≤–∞–µ–º —Ç—Ä–µ–∫")
                    tracked_hand = None
                    tracked_trajectory = []
                    last_tracked_bbox = None
                    last_tracked_trajectory = []

    if last_tracked_bbox:
        x1, y1, x2, y2 = last_tracked_bbox
        cv2.rectangle(color_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(color_frame, "Tracking", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    for i in range(1, len(last_tracked_trajectory)):
        cv2.line(color_frame, last_tracked_trajectory[i - 1], last_tracked_trajectory[i], (0, 0, 255), 2)

    return color_frame


def get_frames():
    frames: FrameSet = pipeline.wait_for_frames(500)
    if frames is None:
        return None, None

    color_frame = frames.get_color_frame()
    if color_frame is None:
        return None, None
    color_image = frame_to_bgr_image(color_frame)

    depth_frame = frames.get_depth_frame()
    if depth_frame is None:
        return None, None
    depth_data = (np.frombuffer(depth_frame.get_data().copy(order='C'), dtype=np.uint16).copy(order='C')
                  .reshape((depth_frame.get_height(), depth_frame.get_width())))

    return color_image, depth_data


def visualize_depth(depth_frame):
    depth_normalized = cv2.normalize(depth_frame, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)
    depth_colormap = cv2.applyColorMap(depth_normalized, cv2.COLORMAP_JET)
    return depth_colormap


while True:
    color_frame, depth_frame = get_frames()
    if color_frame is None or depth_frame is None:
        continue

    processed_frame = detect_and_track(color_frame, depth_frame)
    depth_colormap = visualize_depth(depth_frame)

    combined_view = np.hstack((processed_frame, depth_colormap))

    cv2.imshow('Sign Recognition', combined_view)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

pipeline.stop()
cv2.destroyAllWindows()
